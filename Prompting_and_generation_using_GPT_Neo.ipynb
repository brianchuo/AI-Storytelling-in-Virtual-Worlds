{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianchuo/AI-Storytelling-in-Virtual-Worlds/blob/main/Prompting_and_generation_using_GPT_Neo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsbGlYib3cPG"
      },
      "source": [
        "# Prompting and Generation using GPT-Neo.\n",
        "\n",
        "In this notebook, you will experiment with language generation and controlling outputs of a pre-trained model without any additional training or fine-tuning. \n",
        "\n",
        "We will be utilizing the [Huggingface transformers library](https://huggingface.co/docs/transformers/index); this library gives us access to many pre-trained language models with a simple interface that allows us to generate text in as little as four lines of code. \n",
        "\n",
        "In the first part of the notebook, you will implement a function to generate several candidate sentences for an input prompt. Then, you will let the user to select one of the candidate sentences to be the following input prompt, giving the user control over the generated text.\n",
        "\n",
        "In the second part of the notebook, we will automate the candidate selection by scoring the sentences for toxicity and returning the least toxic sentence to be the following prompt.\n",
        "\n",
        "In the last and final part of the notebook, you will propose your scoring function to elect which sentence to return. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFUSuIFfGGxI"
      },
      "source": [
        "# Installations\n",
        "\n",
        "In this section, we will install the required libraries. Before running any code, make sure that you select GPU in your runtime environment. The code in this section needs only to be run once at the Colab environment's initialization. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxUAN5SW4QTT"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKVn1QxM2RKM"
      },
      "outputs": [],
      "source": [
        "!pip install detoxify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1qcuhadGM_c"
      },
      "source": [
        "# Imports \n",
        "In this section, we will have our package imports. Importing all the packages/libraries, we will be using beforehand. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LvtnIZ3GPXb"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "import torch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu0mpHti2cCt"
      },
      "outputs": [],
      "source": [
        "from detoxify import Detoxify"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Model and Tokenizer"
      ],
      "metadata": {
        "id": "60coUbwuHsVB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xua4w9YyGWJL"
      },
      "outputs": [],
      "source": [
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify that there is a GPU assigned to this notebook runtime environment."
      ],
      "metadata": {
        "id": "oYgpY5rVLbBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "xGi4rDopLgaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move the model to the GPU"
      ],
      "metadata": {
        "id": "IVQI4AJSORAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to('cuda')"
      ],
      "metadata": {
        "id": "ZS7hCajROOeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iVN669HGTVh"
      },
      "source": [
        "# Part 1: Text Generation with the GPT-Neo Language Model\n",
        "\n",
        "## 1.A.\n",
        "\n",
        "Complete the function below that will generate and return a list of ```num_returned_sentences``` sentences from a given prompt. Each returned sentence should contain the prompt plus ```max_new_tokens``` additional tokens.\n",
        "\n",
        "Consult the [Huggingface Transformers API](https://huggingface.co/docs/transformers/index). Of special importance to this project is the ```generate()``` function call, and the ```Tokenizer``` class.\n",
        "\n",
        "There are generally four steps in generating text with a neural language model from a given prompt. The prompt is a string.\n",
        "\n",
        "1. Encode: convert the string prompt to a tensor of tokens. \n",
        "\n",
        "Each word (or part of a word) that a neural language model recognizes as valid input is a one-hot vector. Since a one-hot vector is a vector the size of the recognized vocabulary with a single bit turned on, each word (or part of a word) is equivalent to the index of that bit in the vocabular. Thus each word (or part of a word) can be transformed into an integer, called a token. The ```tokenizer``` object knows the language model's vocabulary.\n",
        "\n",
        "2. Pass the tokenized prompt into the language model to generate a distribution over successor tokens.\n",
        "\n",
        "The distribution for the succesor token is the negative log likelihood of each possible token given the prior tokens p(token_n | token_1, ... token_{n-1}) according to the model. Since we will be sampling from this distribution, we might get some low probability tokens. To improve quality, we can truncate the head of the distribution in two ways. First, we can keep the top k tokens of the distribution and zero out the rest of the distribution. Second, we can keep p% of the distribution mass. \n",
        "\n",
        "3. Sample from the distribution to construct one or more sequences of tokens, typically in the form of a multidimensional tensor.\n",
        "\n",
        "There are many ways to sample from this distribution. For example, greedy sampling is to take the top best token. Multinomial sampling randomly selects a token based on the distribution. ```model.generate()``` can be used to combine steps 2 and 3.\n",
        "\n",
        "4. Decode: convert sampled tokens back into words.\n",
        "\n",
        "Use the ```tokenizer``` object again to convert the tensors of tokens into strings. \n",
        "\n",
        "Below you will implement those 4 steps by completing the following method ```generate_candidate_sentences```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqRcvsKFz08b"
      },
      "outputs": [],
      "source": [
        "def generate_candidate_sentences(model, tokenizer, prompt, k, num_returned_sentences, max_new_tokens=20):\n",
        "  # This function will return num_returned_sentences based on a given prompt.\n",
        "  # Each sentence should be max_new_tokens longer than the prompt.\n",
        "  # model: the GPT-neo model \n",
        "  # tokenizer: the object that tokenizes and de-tokenizes text\n",
        "  # prompt: a string\n",
        "  # k: stochastically sample sequences from the top k sequences in the distribution generated by the model.\n",
        "  #    A larger k gives more variability and variety.\n",
        "  #    This must be equal to or larger than num_returned_sentences\n",
        "  # num_returned_sentences: the number of sentences to return from this function\n",
        "  # max_length: the maximum length of the output sentences (in terms of number of tokens)\n",
        "  ### STEP 1: Use the tokenizer to convert the prompt into a tensor of tokens\n",
        "  ###         (don't forget to move the tensor to the GPU)\n",
        "  ### STEP 2 and 3: Call model.generate and receive a tensor of length num_returned_sentences\n",
        "  ### STEP 4: Convert the tensor into a list of num_returned_sentences to a list of \n",
        "  output_sentences = [] # A list of output sentences to collect up and return\n",
        "  ### TODO: WRITE YOUR CODE BELOW HERE:\n",
        "\n",
        "  ### WRITE YOUR CODE ABOVE HERE\n",
        "  return output_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##1.B\n",
        "Test your ```generate_candidate_sentences()``` here"
      ],
      "metadata": {
        "id": "uhgqJuOXDb8y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcEcywatzft5"
      },
      "outputs": [],
      "source": [
        "k = 5\n",
        "num_returned_sentences = 5\n",
        "text_length = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN-ciQ-X6JO_"
      },
      "outputs": [],
      "source": [
        "def print_candidate_sentences(candidate_sentences):\n",
        "  print(\"Output:\\n\" + 100 * '-')\n",
        "  for i, output in enumerate(candidate_sentences):\n",
        "    print(\"{}: {} \\n\".format(i, output) + 20*'-')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ07DicrGZ5N"
      },
      "outputs": [],
      "source": [
        "prompt = \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \" \\\n",
        "\"previously unexplored valley, in the Andes Mountains. Even more surprising to the \" \\\n",
        "\"researchers was the fact that the unicorns spoke perfect English.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sequences = generate_candidate_sentences(model, tokenizer, prompt, k, num_returned_sentences, text_length)\n",
        "print_candidate_sentences(generated_sequences)"
      ],
      "metadata": {
        "id": "HF3PhpgH7Elp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.C\n",
        "Large pre-trained language models have some limitations that we sometimes need to work around to utilize them effectively for our tasks. One of these limitations is the maximum token length, where these models can generate a capped number of tokens on each call to the ``` model.generate()``` function. For GPT-2, the maximum number of tokens is 1024, and for GPT-Neo, it is 2048. This limit is not practical as it restricts the generation of longer sequences of texts such as articles and stories. \n",
        "\n",
        "\n",
        "For 1.C: \n",
        "To account for this limitation and succesfully handle longer text generation; we can iteratively call the generate function, truncating the input prompt to make sure the input+generated text are less than the maximum token length.\n",
        "\n",
        "Then, iteratively, given a prompt, generate a story longer than the 2048 token limit. \n",
        "\n",
        "---------------\n",
        "Tip: For ease of testing, you can set the ```max_length``` parameter in the ```model.generate()``` function to 10 and try to use your code to generate a story of length 50. Setting this parameter can mimic the model's behavior when it receives a much larger input than expected. "
      ],
      "metadata": {
        "id": "GAJFiRgeDmqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "target_length = 4096"
      ],
      "metadata": {
        "id": "Qzh7Ih1po0PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \" \\\n",
        "\"previously unexplored valley, in the Andes Mountains. Even more surprising to the \" \\\n",
        "\"researchers was the fact that the unicorns spoke perfect English.\""
      ],
      "metadata": {
        "id": "Yb0ABwk5XEMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_long(model, tokenizer, prompt, k, target_length):\n",
        "  # Use the generate_candidate_sentences iteratively to generate longer sequences \n",
        "  # Step 1: Check the length of the current prompt, truncate it if it will result in text that is be longer than max length threshold. \n",
        "  # Make sure your new prompt substring includes the end of the prompt so the model can correctly produce a continuation.  \n",
        "  # Step 2: Call the generate_candidate_sentences with the new substring prompt (Note, num_returned_sentences can be set to 1 for this section)\n",
        "  # Step 3: Update the full_generated text to include the full prompt and the newly generated content.\n",
        "  # Step 4: Repeat steps 1-3 until the goal length is reached. \n",
        "  full_generated_text = prompt\n",
        "  ### TODO: WRITE YOUR CODE BELOW HERE\n",
        "\n",
        "  ### WRITE YOUR CODE ABOVE HERE\n",
        "  return full_generated_text"
      ],
      "metadata": {
        "id": "Oi6MLYEa456Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full = generate_long(model, tokenizer, prompt, k, target_length)\n",
        "print(full)"
      ],
      "metadata": {
        "id": "3qpGNxAwVdYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ptq7l-9u74"
      },
      "source": [
        "# Part 2: Text generation on user preference\n",
        "\n",
        "For this section you will iteratively generate text based on user preferences. Generating a few candidate continuations for the user to select from at each iteration and then using the selected continuation as a prompt for the next segment.\n",
        "\n",
        "The behavior of the function is as follows. It should produce ```num_choices``` possible candidates. Only print out only the newly generated text. The user will provide a number indicating which of the candidates they want to use (see ```get_user_preference``` for a useful utility). The selected candidate will be added to the prompt. This will be done ```num_sentences_to_generate``` times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvVhYquj59gy"
      },
      "outputs": [],
      "source": [
        "def get_user_preference(candidate_sentences):\n",
        "  while True:\n",
        "    try: \n",
        "      user_choice = int(input(\"Enter the sentence number: \"))\n",
        "      if user_choice >= 0 and user_choice < len(candidate_sentences):\n",
        "          return user_choice\n",
        "      else:\n",
        "        raise ValueError\n",
        "    except ValueError: \n",
        "      print(\"Please enter a correct sentence number between {} and {}\".format(0, len(candidate_sentences)-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "num_choices = 5\n",
        "text_length = 20\n",
        "num_sentences_to_generate = 5"
      ],
      "metadata": {
        "id": "_AKrs2E2ne0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \" \\\n",
        "\"previously unexplored valley, in the Andes Mountains. Even more surprising to the \" \\\n",
        "\"researchers was the fact that the unicorns spoke perfect English.\""
      ],
      "metadata": {
        "id": "uq6FP-K7XF25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start with the prompt. Generate ```num_choices``` continuations. Have the user choose the sentence that they want, and return that sentence"
      ],
      "metadata": {
        "id": "Std5xg3-Tked"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvcRtJ6U0P82"
      },
      "outputs": [],
      "source": [
        "def generate_sentence_interactive(model, tokenizer, prompt, k, num_choices, num_sentences_to_generate, max_new_tokens=text_length):\n",
        "  # We will generate the candidate sentences for the next prompt and and then get the index of the user's pereference. \n",
        "  # Then we use this to assign a new prompt to our model:\n",
        "  # STEP 1: Generate candidate sentences.\n",
        "  # STEP 2: Get the user's preference.\n",
        "  # STEP 3: Map user choice to a sentence and then use this sentence (added to the prior context) to generate the next candidates.\n",
        "  final_sentence = None\n",
        "  ### TODO: WRITE YOUR CODE BELOW HERE\n",
        "\n",
        "  ### WRITE YOUR CODE ABOVE HERE\n",
        "  return final_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check your final output here"
      ],
      "metadata": {
        "id": "tWqHCg1YsB4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nFhdcZfHqYG"
      },
      "outputs": [],
      "source": [
        "continuation = generate_sentence_interactive(model, tokenizer, prompt, k, num_choices, num_sentences_to_generate, text_length)\n",
        "print(\"Final output: \\n {}\".format(continuation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILQ105ycr-HV"
      },
      "source": [
        "# Part 3: Text generation with automated toxicity scoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nI6hAXfGbJr"
      },
      "source": [
        "In this part we will use [Detoxify](https://github.com/unitaryai/detoxify) scoring to choose the least toxic next sentence. \n",
        "\n",
        "First, run the code cell below to get an idea of how Detoxify works and how the output of the predict function looks like. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kcib07h7k9a"
      },
      "outputs": [],
      "source": [
        "# In this cell we show a demo for the Detoxify model. \n",
        "# You can use the Detoxify model \n",
        "detoxify_model = Detoxify('unbiased') \n",
        "results = detoxify_model.predict(['example text 1','example text 2'])\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggecya_32lke"
      },
      "outputs": [],
      "source": [
        "k = 5\n",
        "num_choices = 5\n",
        "text_length = 20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \" \\\n",
        "\"previously unexplored valley, in the Andes Mountains. Even more surprising to the \" \\\n",
        "\"researchers was the fact that the unicorns spoke perfect English.\""
      ],
      "metadata": {
        "id": "_bwd8wUoXG0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, use the Detoxify model to rank the generated candidates and return the full text with the lowest ```toxicity```."
      ],
      "metadata": {
        "id": "WJLuOiVCtSBG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-ty2CE62qhE"
      },
      "outputs": [],
      "source": [
        "def generate_detox_sentence(model, tokenizer, prompt, k, num_choices, max_new_tokens=text_length):\n",
        "  # STEP 1: Generate candidate texts.\n",
        "  # STEP 2: Find the candidate with the lowest toxicity score. \n",
        "  # STEP 3: Return the full text with the lowest toxicity score.\n",
        "  best_sentence = None\n",
        "  ### TODO: WRITE YOUR CODE BELOW HERE\n",
        "\n",
        "  ### WRITE YOUR CODE ABOVE HERE\n",
        "  return best_sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "continuation = generate_detox_sentence(model, tokenizer, prompt, k, num_choices, text_length)\n",
        "print(continuation)"
      ],
      "metadata": {
        "id": "OCakqKC-m3O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVPJbe4D8YJh"
      },
      "source": [
        "# Part 4: Text generation with other scoring functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MbYDyE-8elG"
      },
      "source": [
        "For this part we want you to come up with your own scoring mechanism. Choose a method the you think would improve story generation process using the pretrained model. Demonestrate it and explain briefly why you think it is a suitable scoring function? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "348CezvN8ebu"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Generate candidate text.\n",
        "# STEP 2: Use your scoring function on the generated candidates \n",
        "# STEP 3: Return the full text with the lowest toxicity score.\n",
        "### TODO: WRITE YOUR CODE BELOW HERE\n",
        "\n",
        "### WRITE YOUR CODE ABOVE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR EXPLANATION HERE:** ..."
      ],
      "metadata": {
        "id": "n22aefCEsa7b"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Prompting and generation using GPT-Neo.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}